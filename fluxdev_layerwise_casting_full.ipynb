{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e57f22-0f0d-40e1-8c42-cdc500ce0d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple distributions found for package optimum. Picked distribution: optimum\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e686e1b90822449098239eaea6538b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4cd33381ea4b5080431551bc64180c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded 3.71 secs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5205e8db26974ef58de8de8cfd825506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f95fcc426649159f7f52eead586952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9da014feb014a0183af7fe56161ce53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max VRAM Peak: 22.19 GBMem used / temp C / utilized: 20490 MiB, 54, 64 %\n",
      "\n",
      "Inference time: 11.99 secs, total time: 20.13 secs\n"
     ]
    }
   ],
   "source": [
    "from diffusers import FluxPipeline, FluxTransformer2DModel\n",
    "from transformers import T5EncoderModel\n",
    "import torch, time, os, subprocess, gc\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "prompt = \"\"\"\n",
    "Skeuomorphism,gobo lighting,long exposure, photorealistic close-up of an iridescent hummingbird hovering mid-air, drinking nectar from a bioluminescent flower, a dewdrop clings precariously to a spiderweb woven of pure silver. land art\n",
    "\"\"\"\n",
    "\n",
    "width = 1536\n",
    "height = 512\n",
    "guidance_scale = 2.5\n",
    "num_inference_steps = 20\n",
    "\n",
    "t1_start, dtype, model_id = time.time(), torch.bfloat16, \"black-forest-labs/FLUX.1-dev\"\n",
    "float8_storage_dtype = torch.float8_e4m3fn #or float8_e5m2\n",
    "\n",
    "t5_encoder = T5EncoderModel.from_pretrained(model_id, subfolder=\"text_encoder_2\", torch_dtype=dtype).to(\"cuda\")\n",
    "text_encoder = FluxPipeline.from_pretrained(model_id, text_encoder_2=t5_encoder, transformer=None, vae=None, torch_dtype=dtype).to(\"cuda\") \n",
    "\n",
    "with torch.no_grad():\n",
    "    prompt_embeds, pooled_prompt_embeds, _ = text_encoder.encode_prompt(prompt=prompt, prompt_2=prompt, max_sequence_length=512)\n",
    "\n",
    "embeddings_prompt_embeds = prompt_embeds.detach().cpu()\n",
    "embeddings_pooled_prompt_embeds = pooled_prompt_embeds.detach().cpu()\n",
    "\n",
    "del text_encoder\n",
    "del t5_encoder\n",
    "flush()\n",
    "print(f\"Encoded {time.time() - t1_start:.2f} secs\")\n",
    "\n",
    "t2_start = time.time()\n",
    "transformer = FluxTransformer2DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=dtype).to(\"cuda\")\n",
    "transformer.enable_layerwise_casting(storage_dtype=float8_storage_dtype)\n",
    "flush() \n",
    "\n",
    "pipeline = FluxPipeline.from_pretrained(model_id, torch_dtype=dtype, transformer=transformer, text_encoder_2=None, text_encoder=None, tokenizer_2=None, tokenizer=None).to(\"cuda\")\n",
    "\n",
    "prompt_embeds_gen = embeddings_prompt_embeds.to(\"cuda\").to(dtype)\n",
    "pooled_prompt_embeds_gen = embeddings_pooled_prompt_embeds.to(\"cuda\").to(dtype)\n",
    "flush()\n",
    "\n",
    "gen_start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    output = pipeline(prompt_embeds=prompt_embeds_gen, pooled_prompt_embeds=pooled_prompt_embeds_gen, width=width, height=height, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps)\n",
    "\n",
    "image, timestamp = output.images[0], time.time()\n",
    "filename = f\"flux_cfg{guidance_scale}_steps{num_inference_steps}_layerwise_{float8_storage_dtype}_{timestamp}.png\"\n",
    "image.save(filename); os.startfile(filename) # Windows specific\n",
    "\n",
    "result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "print(f\"Max VRAM Peak: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GBMem used / temp C / utilized: {result}\")\n",
    "print(f\"Inference time: {timestamp - gen_start_time:.2f} secs, total time: {time.time() - t2_start:.2f} secs\")\n",
    "\n",
    "del pipeline;del transformer;del prompt_embeds_gen;del pooled_prompt_embeds_gen\n",
    "flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537ef34-3182-416c-ab77-a28b016f8237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
