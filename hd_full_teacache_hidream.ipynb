{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627fad0-2d2b-46fc-b8e3-bd2502270584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, gc, time, threading, subprocess, psutil, shutil\n",
    "import numpy as np \n",
    "from typing import Any, Dict, List, Optional, Tuple \n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaForCausalLM, T5EncoderModel, T5Tokenizer, CLIPTextModelWithProjection, CLIPTokenizer\n",
    "from diffusers import HiDreamImagePipeline, HiDreamImageTransformer2DModel, UniPCMultistepScheduler, AutoencoderKL, BitsAndBytesConfig as DiffusersBitsAndBytesConfig\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.models.modeling_outputs import Transformer2DModelOutput \n",
    "from diffusers.utils import logging, deprecate, USE_PEFT_BACKEND, scale_lora_layers, unscale_lora_layers \n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)\n",
    "\n",
    "def teacache_forward(\n",
    "    self,\n",
    "    hidden_states: torch.Tensor,\n",
    "    timesteps: torch.LongTensor = None,\n",
    "    encoder_hidden_states_t5: torch.Tensor = None,\n",
    "    encoder_hidden_states_llama3: torch.Tensor = None,\n",
    "    pooled_embeds: torch.Tensor = None,\n",
    "    img_ids: Optional[torch.Tensor] = None,\n",
    "    img_sizes: Optional[List[Tuple[int, int]]] = None,\n",
    "    hidden_states_masks: Optional[torch.Tensor] = None,\n",
    "    attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    return_dict: bool = True,\n",
    "    **kwargs,\n",
    "):\n",
    "    encoder_hidden_states = kwargs.get(\"encoder_hidden_states\", None)\n",
    "\n",
    "    if encoder_hidden_states is not None:\n",
    "        deprecation_message = \"The `encoder_hidden_states` argument is deprecated. Please use `encoder_hidden_states_t5` and `encoder_hidden_states_llama3` instead.\"\n",
    "        deprecate(\"encoder_hidden_states\", \"0.35.0\", deprecation_message)\n",
    "        encoder_hidden_states_t5 = encoder_hidden_states[0]\n",
    "        encoder_hidden_states_llama3 = encoder_hidden_states[1]\n",
    "\n",
    "    if img_ids is not None and img_sizes is not None and hidden_states_masks is None:\n",
    "        deprecation_message = (\n",
    "            \"Passing `img_ids` and `img_sizes` with unpachified `hidden_states` is deprecated and will be ignored.\"\n",
    "        )\n",
    "        deprecate(\"img_ids\", \"0.35.0\", deprecation_message)\n",
    "\n",
    "    if hidden_states_masks is not None and (img_ids is None or img_sizes is None):\n",
    "        raise ValueError(\"if `hidden_states_masks` is passed, `img_ids` and `img_sizes` must also be passed.\")\n",
    "    elif hidden_states_masks is not None and hidden_states.ndim != 3:\n",
    "        raise ValueError(\n",
    "            \"if `hidden_states_masks` is passed, `hidden_states` must be a 3D tensors with shape (batch_size, patch_height * patch_width, patch_size * patch_size * channels)\"\n",
    "        )\n",
    "\n",
    "    if attention_kwargs is not None:\n",
    "        attention_kwargs = attention_kwargs.copy()\n",
    "        lora_scale = attention_kwargs.pop(\"scale\", 1.0)\n",
    "    else:\n",
    "        lora_scale = 1.0\n",
    "\n",
    "    if USE_PEFT_BACKEND:\n",
    "        scale_lora_layers(self, lora_scale)\n",
    "    else:\n",
    "        if attention_kwargs is not None and attention_kwargs.get(\"scale\", None) is not None:\n",
    "            logger.warning(\n",
    "                \"Passing `scale` via `attention_kwargs` when not using the PEFT backend is ineffective.\"\n",
    "            )\n",
    "\n",
    "    batch_size = hidden_states.shape[0]\n",
    "    hidden_states_type = hidden_states.dtype\n",
    "\n",
    "    if hidden_states_masks is None:\n",
    "        hidden_states, hidden_states_masks, img_sizes, img_ids = self.patchify(hidden_states)\n",
    "\n",
    "    hidden_states = self.x_embedder(hidden_states)\n",
    "\n",
    "    timesteps_embed = self.t_embedder(timesteps, hidden_states_type)\n",
    "    p_embedder = self.p_embedder(pooled_embeds)\n",
    "    temb = timesteps_embed + p_embedder\n",
    "\n",
    "    encoder_hidden_states = [encoder_hidden_states_llama3[k] for k in self.config.llama_layers]\n",
    "\n",
    "    if self.caption_projection is not None:\n",
    "        new_encoder_hidden_states = []\n",
    "        for i, enc_hidden_state in enumerate(encoder_hidden_states):\n",
    "            enc_hidden_state = self.caption_projection[i](enc_hidden_state)\n",
    "            enc_hidden_state = enc_hidden_state.view(batch_size, -1, hidden_states.shape[-1])\n",
    "            new_encoder_hidden_states.append(enc_hidden_state)\n",
    "        encoder_hidden_states = new_encoder_hidden_states\n",
    "        encoder_hidden_states_t5 = self.caption_projection[-1](encoder_hidden_states_t5)\n",
    "        encoder_hidden_states_t5 = encoder_hidden_states_t5.view(batch_size, -1, hidden_states.shape[-1])\n",
    "        encoder_hidden_states.append(encoder_hidden_states_t5)\n",
    "\n",
    "    txt_ids = torch.zeros(\n",
    "        batch_size,\n",
    "        encoder_hidden_states[-1].shape[1]\n",
    "        + encoder_hidden_states[-2].shape[1]\n",
    "        + encoder_hidden_states[0].shape[1],\n",
    "        3,\n",
    "        device=img_ids.device,\n",
    "        dtype=img_ids.dtype,\n",
    "    )\n",
    "    ids = torch.cat((img_ids, txt_ids), dim=1)\n",
    "    image_rotary_emb = self.pe_embedder(ids)\n",
    "\n",
    "    block_id = 0\n",
    "    initial_encoder_hidden_states = torch.cat([encoder_hidden_states[-1], encoder_hidden_states[-2]], dim=1)\n",
    "    initial_encoder_hidden_states_seq_len = initial_encoder_hidden_states.shape[1]\n",
    "\n",
    "    should_calc = True\n",
    "    if self.enable_teacache:\n",
    "        modulated_inp = timesteps_embed.clone()\n",
    "        if self.cnt < self.ret_steps:\n",
    "            should_calc = True\n",
    "            self.accumulated_rel_l1_distance = 0\n",
    "        else:\n",
    "            distance = ((modulated_inp - self.previous_modulated_input).abs().mean() / self.previous_modulated_input.abs().mean()).cpu().item()\n",
    "            rescale_func = np.poly1d(self.coefficients)\n",
    "            self.accumulated_rel_l1_distance += rescale_func(distance)\n",
    "            if self.accumulated_rel_l1_distance < self.rel_l1_thresh:\n",
    "                should_calc = False\n",
    "            else:\n",
    "                should_calc = True\n",
    "                self.accumulated_rel_l1_distance = 0\n",
    "        self.previous_modulated_input = modulated_inp \n",
    "        self.cnt += 1\n",
    "        if self.cnt == self.num_steps:\n",
    "            self.cnt = 0\n",
    "\n",
    "    if self.enable_teacache and not should_calc:\n",
    "        hidden_states += self.previous_residual\n",
    "    else:\n",
    "        ori_hidden_states = hidden_states.clone()\n",
    "        for bid, block in enumerate(self.double_stream_blocks):\n",
    "            cur_llama31_encoder_hidden_states = encoder_hidden_states[block_id]\n",
    "            cur_encoder_hidden_states = torch.cat(\n",
    "                [initial_encoder_hidden_states, cur_llama31_encoder_hidden_states], dim=1\n",
    "            )\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                hidden_states, initial_encoder_hidden_states = self._gradient_checkpointing_func(\n",
    "                    block, hidden_states, hidden_states_masks, cur_encoder_hidden_states, temb, image_rotary_emb,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states, initial_encoder_hidden_states = block(\n",
    "                    hidden_states=hidden_states, hidden_states_masks=hidden_states_masks,\n",
    "                    encoder_hidden_states=cur_encoder_hidden_states, temb=temb, image_rotary_emb=image_rotary_emb,\n",
    "                )\n",
    "            initial_encoder_hidden_states = initial_encoder_hidden_states[:, :initial_encoder_hidden_states_seq_len]\n",
    "            block_id += 1\n",
    "\n",
    "        image_tokens_seq_len = hidden_states.shape[1]\n",
    "        hidden_states = torch.cat([hidden_states, initial_encoder_hidden_states], dim=1)\n",
    "        hidden_states_seq_len = hidden_states.shape[1]\n",
    "        if hidden_states_masks is not None:\n",
    "            encoder_attention_mask_ones = torch.ones(\n",
    "                (batch_size, initial_encoder_hidden_states.shape[1] + cur_llama31_encoder_hidden_states.shape[1]),\n",
    "                device=hidden_states_masks.device, dtype=hidden_states_masks.dtype,\n",
    "            )\n",
    "            hidden_states_masks = torch.cat([hidden_states_masks, encoder_attention_mask_ones], dim=1)\n",
    "\n",
    "        for bid, block in enumerate(self.single_stream_blocks):\n",
    "            cur_llama31_encoder_hidden_states = encoder_hidden_states[block_id]\n",
    "            hidden_states = torch.cat([hidden_states, cur_llama31_encoder_hidden_states], dim=1)\n",
    "            if torch.is_grad_enabled() and self.gradient_checkpointing:\n",
    "                hidden_states = self._gradient_checkpointing_func(\n",
    "                    block, hidden_states, hidden_states_masks, None, temb, image_rotary_emb,\n",
    "                )\n",
    "            else:\n",
    "                hidden_states = block(\n",
    "                    hidden_states=hidden_states, hidden_states_masks=hidden_states_masks,\n",
    "                    encoder_hidden_states=None, temb=temb, image_rotary_emb=image_rotary_emb,\n",
    "                )\n",
    "            hidden_states = hidden_states[:, :hidden_states_seq_len]\n",
    "            block_id += 1\n",
    "\n",
    "        hidden_states = hidden_states[:, :image_tokens_seq_len, ...]\n",
    "        \n",
    "        if self.enable_teacache:\n",
    "            self.previous_residual = hidden_states - ori_hidden_states\n",
    "\n",
    "    output = self.final_layer(hidden_states, temb)\n",
    "    output = self.unpatchify(output, img_sizes, self.training)\n",
    "    if hidden_states_masks is not None:\n",
    "        hidden_states_masks = hidden_states_masks[:, :image_tokens_seq_len]\n",
    "\n",
    "    if USE_PEFT_BACKEND:\n",
    "        unscale_lora_layers(self, lora_scale)\n",
    "\n",
    "    if not return_dict:\n",
    "        return (output,)\n",
    "    return Transformer2DModelOutput(sample=output)\n",
    "\n",
    "\n",
    "width=1024\n",
    "height=1024\n",
    "\n",
    "cfg, steps, model_repo_id = 3.5, 50, \"HiDream-ai/HiDream-I1-Full\"\n",
    "llama_repo_id = \"John6666/Llama-3.1-8B-Lexi-Uncensored-V2-nf4\"\n",
    "\n",
    "TEMP_EMBED_DIR = \"temp_embeddings\"\n",
    "tick_begins = time.time()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "def memory():\n",
    "    try:\n",
    "        gi = subprocess.run(['nvidia-smi','--query-gpu=pstate,memory.used,temperature.gpu,utilization.gpu',\n",
    "                             '--format=csv,noheader'], capture_output=True, text=True, check=True).stdout.strip().split(',')\n",
    "        vu, ps, util, temp, ram = float(gi[1].strip().replace(\" MiB\", \"\")) / 1024, gi[0].strip(), gi[3].strip(), gi[2].strip(), psutil.virtual_memory()\n",
    "        print(f\"   ...\" + \"\\033[96m\" + f\"VRAM:\" + \"\\033[93m\" + f\"{vu:.1f}\" +  \"\\033[96m\" + f\"/24GB \" + \\\n",
    "              \"\\033[93m\" + f\"{ps} \" + \"\\033[96m\" + f\"{util} {temp}C | RAM:\" + \"\\033[93m\" + f\"{ram.used/1024**3:.1f}\" + \"\\033[96m\" + f\"/64GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ...Could not get GPU stats: {e}\")\n",
    "\n",
    "def flush_vram():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def save_and_open_image(image, timestamp):\n",
    "    filename = f\"{timestamp}_hidream_teacache.png\"\n",
    "    image.save(filename)\n",
    "    print(f\"\\n   ...Saved image to {filename}\")\n",
    "    os.startfile(filename)\n",
    "\n",
    "# === PHASE 1 (UNCHANGED) ===\n",
    "def precompute_and_save_embeddings(prompts, negative_prompt, temp_dir):\n",
    "    print(f\"{'='*10} PHASE 1: Pre-computing and Saving Embeddings {'='*10}\")\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"   ...Loading all text encoders...\")\n",
    "    tokenizer_1 = CLIPTokenizer.from_pretrained(model_repo_id, subfolder=\"tokenizer\")\n",
    "    text_encoder_1 = CLIPTextModelWithProjection.from_pretrained(model_repo_id, subfolder=\"text_encoder\", torch_dtype=dtype).to(device)\n",
    "    tokenizer_2 = CLIPTokenizer.from_pretrained(model_repo_id, subfolder=\"tokenizer_2\")\n",
    "    text_encoder_2 = CLIPTextModelWithProjection.from_pretrained(model_repo_id, subfolder=\"text_encoder_2\", torch_dtype=dtype).to(device)\n",
    "    tokenizer_3 = T5Tokenizer.from_pretrained(model_repo_id, subfolder=\"tokenizer_3\")\n",
    "    text_encoder_3 = T5EncoderModel.from_pretrained(model_repo_id, subfolder=\"text_encoder_3\", torch_dtype=dtype).to(device)\n",
    "    tokenizer_4 = AutoTokenizer.from_pretrained(llama_repo_id)\n",
    "    tokenizer_4.pad_token = tokenizer_4.eos_token\n",
    "    text_encoder_4 = LlamaForCausalLM.from_pretrained(llama_repo_id, output_hidden_states=True, torch_dtype=dtype).to(device)\n",
    "    print(\"   ...All encoders loaded.\"); memory()\n",
    "\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"   ...Encoding prompt {i+1}/{len(prompts)}...\")\n",
    "        with torch.inference_mode():\n",
    "            prompt_list = [prompt]\n",
    "            neg_prompt_list = [negative_prompt]\n",
    "            prompt_embeds_1 = text_encoder_1(tokenizer_1(prompt_list, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device))[0]\n",
    "            prompt_embeds_2 = text_encoder_2(tokenizer_2(prompt_list, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device))[0]\n",
    "            pooled_prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)\n",
    "            neg_prompt_embeds_1 = text_encoder_1(tokenizer_1(neg_prompt_list, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device))[0]\n",
    "            neg_prompt_embeds_2 = text_encoder_2(tokenizer_2(neg_prompt_list, padding=\"max_length\", max_length=77, truncation=True, return_tensors=\"pt\").input_ids.to(device))[0]\n",
    "            negative_pooled_prompt_embeds = torch.cat([neg_prompt_embeds_1, neg_prompt_embeds_2], dim=-1)\n",
    "            prompt_tokens_3 = tokenizer_3(prompt_list, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "            prompt_embeds_t5 = text_encoder_3(prompt_tokens_3.input_ids.to(device), attention_mask=prompt_tokens_3.attention_mask.to(device))[0]\n",
    "            neg_prompt_tokens_3 = tokenizer_3(neg_prompt_list, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "            negative_prompt_embeds_t5 = text_encoder_3(neg_prompt_tokens_3.input_ids.to(device), attention_mask=neg_prompt_tokens_3.attention_mask.to(device))[0]\n",
    "            prompt_tokens_4 = tokenizer_4(prompt_list, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "            prompt_outputs = text_encoder_4(prompt_tokens_4.input_ids.to(device), attention_mask=prompt_tokens_4.attention_mask.to(device))\n",
    "            prompt_embeds_llama3 = torch.stack(prompt_outputs.hidden_states[1:], dim=0)\n",
    "            neg_prompt_tokens_4 = tokenizer_4(neg_prompt_list, padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "            neg_prompt_outputs = text_encoder_4(neg_prompt_tokens_4.input_ids.to(device), attention_mask=neg_prompt_tokens_4.attention_mask.to(device))\n",
    "            negative_prompt_embeds_llama3 = torch.stack(neg_prompt_outputs.hidden_states[1:], dim=0)\n",
    "            single_embeds = {\n",
    "                \"prompt_embeds_t5\": prompt_embeds_t5.cpu(), \"prompt_embeds_llama3\": prompt_embeds_llama3.cpu(),\n",
    "                \"pooled_prompt_embeds\": pooled_prompt_embeds.cpu(), \"negative_prompt_embeds_t5\": negative_prompt_embeds_t5.cpu(),\n",
    "                \"negative_prompt_embeds_llama3\": negative_prompt_embeds_llama3.cpu(),\n",
    "                \"negative_pooled_prompt_embeds\": negative_pooled_prompt_embeds.cpu(),\n",
    "            }\n",
    "            torch.save(single_embeds, os.path.join(temp_dir, f\"embed_{i}.pt\"))\n",
    "\n",
    "    print(\"   ...Purging all text encoders from memory...\")\n",
    "    del text_encoder_1, text_encoder_2, text_encoder_3, text_encoder_4, tokenizer_1, tokenizer_2, tokenizer_3, tokenizer_4\n",
    "    flush_vram()\n",
    "    print(f\"{'='*10} PHASE 1 COMPLETE {'='*10}\"); memory()\n",
    "\n",
    "\n",
    "tick_0 = time.time()\n",
    "prompts_to_generate = [\n",
    "    \"Gorilla in battlearena, his golden breastplate armor shines. He wields a massive black chain, it hangs down at his side, the length and weight a testament to his enormus strength. His fur rich brown, massive chest heaves with contained power, muscles rippled and buldge\",\n",
    "]\n",
    "\n",
    "negative_prompt = \"ugly, blurry, low quality\" # Define the negative prompt\n",
    "\n",
    "\n",
    "try:\n",
    "    precompute_and_save_embeddings(prompts_to_generate, negative_prompt, TEMP_EMBED_DIR)\n",
    "\n",
    "    print(f\"\\n{'='*10} PHASE 2: Generation Loop {'='*10}\")\n",
    "    print(\"   ...Loading generation components (Transformer, VAE)...\")\n",
    "    \n",
    "    quant_config_diffusers = DiffusersBitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type=\"nf4\",bnb_4bit_use_double_quant=True,bnb_4bit_compute_dtype=dtype)\n",
    "    transformer = HiDreamImageTransformer2DModel.from_pretrained(model_repo_id,subfolder=\"transformer\",quantization_config=quant_config_diffusers,torch_dtype=dtype)\n",
    "    vae = AutoencoderKL.from_pretrained(model_repo_id, subfolder=\"vae\", torch_dtype=dtype)\n",
    "    scheduler = UniPCMultistepScheduler(flow_shift=3.0, prediction_type=\"flow_prediction\", use_flow_sigmas=True)\n",
    "    pipe = HiDreamImagePipeline(\n",
    "        vae=vae, transformer=transformer, scheduler=scheduler,\n",
    "        text_encoder=None, tokenizer=None, text_encoder_2=None, tokenizer_2=None,\n",
    "        text_encoder_3=None, tokenizer_3=None, text_encoder_4=None, tokenizer_4=None\n",
    "    ).to(device)\n",
    "    print(\"   ...Generation components loaded and ready.\"); memory()\n",
    "\n",
    "\n",
    "    print(f\"\\n{'='*10} ACTIVATING TEACACHE {'='*10}\")\n",
    "    HiDreamImageTransformer2DModel.forward = teacache_forward\n",
    "    pipe.transformer.__class__.enable_teacache = True\n",
    "    pipe.transformer.__class__.num_steps = steps\n",
    "    pipe.transformer.__class__.ret_steps = int(steps * 0.1) # Warm-up steps\n",
    "    # --- This is the main tuning knob for speed vs. quality ---\n",
    "    REL_L1_THRESH = 0.45  # 0.3 gives ~2x speedup. Higher is faster but may reduce quality.\n",
    "    pipe.transformer.__class__.rel_l1_thresh = REL_L1_THRESH\n",
    "    pipe.transformer.__class__.coefficients = np.array([-3.13605009e+04, -7.12425503e+02, 4.91363285e+01, 8.26515490e+00, 1.08053901e-01])\n",
    "    print(f\"   ...TeaCache enabled with threshold: {REL_L1_THRESH}\")\n",
    "    # =====================================================================================\n",
    "\n",
    "    for i in range(len(prompts_to_generate)):\n",
    "        loop_start_time = time.time()\n",
    "        print(f\"\\n--- Generating image {i+1}/{len(prompts_to_generate)} ---\")\n",
    "        \n",
    "        # Reset TeaCache counter for each new image generation\n",
    "        pipe.transformer.__class__.cnt = 0\n",
    "        \n",
    "        embed_path = os.path.join(TEMP_EMBED_DIR, f\"embed_{i}.pt\")\n",
    "        single_embeds = torch.load(embed_path)\n",
    "        for key in single_embeds:\n",
    "            single_embeds[key] = single_embeds[key].to(device)\n",
    "        print(\"   ...Embeddings loaded into VRAM.\"); memory()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            latents = pipe(\n",
    "                **single_embeds,\n",
    "                guidance_scale=cfg, num_inference_steps=steps, height=height, width=width,\n",
    "                generator=torch.Generator(device).manual_seed(0), output_type=\"latent\",\n",
    "            ).images\n",
    "            print(\"   ...Denoising complete.\"); memory()\n",
    "            \n",
    "            processed_latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "            image_tensor = vae.decode(processed_latents, return_dict=False)[0]\n",
    "            vae_scale_factor = vae.config.scaling_factor * (2 ** (len(vae.config.block_out_channels) - 1))\n",
    "            image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor)\n",
    "            image = image_processor.postprocess(image_tensor, output_type=\"pil\")[0]\n",
    "            \n",
    "            timestamp = int(time.time())\n",
    "            threading.Thread(target=save_and_open_image, args=(image, timestamp)).start()\n",
    "            del single_embeds, latents, image_tensor, image_processor\n",
    "            flush_vram()\n",
    "\n",
    "        print(f\"   ...Image {i+1} completed in {time.time() - loop_start_time:.2f} secs\")\n",
    "\n",
    "finally:\n",
    "    print(f\"\\n{'='*10} PHASE 3: Cleaning up temporary files {'='*10}\")\n",
    "    if os.path.exists(TEMP_EMBED_DIR):\n",
    "        shutil.rmtree(TEMP_EMBED_DIR)\n",
    "        print(f\"   ...Removed temporary directory: {TEMP_EMBED_DIR}\")\n",
    "    print(f\"\\n--- All generations complete. Total script time: {time.time() - tick_0:.2f} secs ---\\n--- ...   {time.time() - tick_begins:.2f} since start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37631022-a3e9-4bde-b698-626094da5838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ultra)",
   "language": "python",
   "name": "ultra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
