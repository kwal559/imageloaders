{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8d13132-0dde-4817-8ebb-e91b47ddb685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd280db465a410b96c700d09cdf9cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea44c9485fcb43fc8fe67a1b9ad6499c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... cog4 Prompts embedded.. 20.48 seconds, Max vram: 18.18 GB\n",
      "   ... Generating 3 Images..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62522e97e1364277812e8f94a50f6a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378208d325504cdca63e502903aff1a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034fd5efb85946689513b3ced9b08e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 23.59 secs, mem/temp/use: 22584 MiB, 70, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d164870096154b88abdd3096cd120442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 27.99 secs, mem/temp/use: 22673 MiB, 72, 97 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b973e4789e4e669a136467fd2ea74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 32.71 secs, mem/temp/use: 22770 MiB, 81, 98 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6331324b03144f578c7f8defe631d002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128c3f3856cd4c768b98120e49ead837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da619cb1f5d945fe9f314b167bbdc999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0552a984d1134d30b65346b0f03c4023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 11.91 secs, mem/temp/use: 16677 MiB, 74, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cae070f1b1e48a2a9a8c485422df534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 14.30 secs, mem/temp/use: 16699 MiB, 83, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d133b392514e8c9953b540ee880331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cd5858103a40d1827dc9ad009cd950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821f4f2af1334db0b986e26eaeb7d765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 19.24 secs, mem/temp/use: 22473 MiB, 82, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe43bbe5e6654a949297cbca976c6d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 23.22 secs, mem/temp/use: 22480 MiB, 84, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c8ca296a9b47ca8f66e62adf6e2d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f72e9e45c240f0ae6f4c4e9e6bcc8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (117 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (117 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fd996009f04a229375ed1bd7217cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 7.24 secs, mem/temp/use: 21975 MiB, 65, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a045063658694f6996149bd8a824f3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 8.01 secs, mem/temp/use: 21975 MiB, 82, 99 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e41900444d94d7094e3059f705a48d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 9.11 secs, mem/temp/use: 21975 MiB, 78, 99 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e3059171f6475e9af7a1a0c76013c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 10.23 secs, mem/temp/use: 21975 MiB, 84, 98 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c3e70dbd2340d3a3e3f1299175466a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d17822cd10c48aeb2cc0533219d6bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... kolors Prompts embedded.. 12.38 seconds, Max vram: 18.18 GB\n",
      "   ... Generating 6 Images..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23d1720585c4e4b8d040d257111a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00ff3b259e9411698ab7e07591f9e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 3.31 secs, mem/temp/use: 11469 MiB, 79, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf9b6daf9c746ab84518a9304d9be9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 3.95 secs, mem/temp/use: 11469 MiB, 75, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c487cb52434b49885f77483b1022a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 4.56 secs, mem/temp/use: 11468 MiB, 73, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1535044e111049eb9d70a107302f3d30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 5.06 secs, mem/temp/use: 11468 MiB, 82, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7017bb9ab4164a42be4cbbc6f3cfcd31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 5.66 secs, mem/temp/use: 11476 MiB, 83, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88280fce096f41c49bc293aa3a51533b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 6.30 secs, mem/temp/use: 11469 MiB, 84, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc05154681545a588029a901439deb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1866047abf794559a484e2f0858452a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (117 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (117 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['filled sky. in the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. the overall atmosphere should be one of serene magic and vibrant detail.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... sd35_large Prompts embedded.. 12.78 seconds, Max vram: 18.18 GB\n",
      "   ... Generating 3 Images..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3af134efae4a598852af63c066d479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c565789666a24026a07c3c5623ba6bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2665bf6c89c44e6e9ecbeae0ec168456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 11.43 secs, mem/temp/use: 22154 MiB, 81, 99 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9f79dd37e147638259944de3e50bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 13.96 secs, mem/temp/use: 22178 MiB, 84, 100 %   ... Max mem: 18.18 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba945333efc64f408d450fa25d32cd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 16.78 secs, mem/temp/use: 22169 MiB, 85, 99 %   ... Max mem: 18.18 GB\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š IMAGE GENERATION BENCHMARK SUMMARY - 2025-04-14 03:58:59\n",
      "================================================================================\n",
      "ðŸ–¼ï¸  Total Images: 20 images across 6 models\n",
      "â±ï¸  Total Runtime: 409.48 seconds (6.82 minutes)\n",
      "ðŸ’¾  Peak VRAM Usage: 18.18 GB\n",
      "âš¡  Avg Generation Time: 12.93 seconds per image\n",
      "ðŸš€  Fastest Model: kolors (4.81 seconds avg)\n",
      "ðŸ¢  Slowest Model: cog4 (28.10 seconds avg)\n",
      "âœ¨  Most Efficient: kolors (0.1282 sec/step)\n",
      "\n",
      "ðŸ“ˆ MODEL COMPARISON:\n",
      "--------------------------------------------------------------------------------\n",
      "Model        | Avg Time (s) | Avg VRAM (GB)  | Time/Step (s)\n",
      "--------------------------------------------------------------------------------\n",
      "cog4         |        28.10 |          18.18 |       0.9367\n",
      "lumina2      |        13.10 |          18.18 |       0.4765\n",
      "aura3        |        21.23 |          18.18 |       0.7721\n",
      "sd35_med     |         8.65 |          18.18 |       0.2306\n",
      "kolors       |         4.81 |          18.18 |       0.1282\n",
      "sd35_large   |        14.06 |          18.18 |       0.5623\n",
      "\n",
      "ðŸŽ® FUN FACTS:\n",
      "  â€¢ The total raw image data generated (39.1 MB) is equivalent to approximately 273.1 floppy disks from the 1990s.\n",
      "  â€¢ Peak VRAM usage of 18.18 GB is approximately 18176518x the total RAM available in the first IBM PC (1981).\n",
      "  â€¢ You generated 20 images in 409.5 seconds - that's 2.9 images per minute!\n",
      "  â€¢ The computational power used would have required a supercomputer the size of a room in the early 2000s.\n",
      "  â€¢ In 1995, this generation would have taken approximately 409479 hours on high-end hardware.\n",
      "\n",
      "ðŸ“ PROMPT:\n",
      "  \"\n",
      "A photorealistic close-up of a single, iridescent hummingbird hovering mid-air, its wings a blur of...\"\n",
      "\n",
      "âœ… Complete! Stats saved to benchmark_stats_[timestamp].json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "A photorealistic close-up of a single, iridescent hummingbird hovering mid-air, its wings a blur of sapphire and emerald, drinking nectar from a luminous, bioluminescent flower that emits soft, swirling particles of golden light. The background is a hyper-detailed, otherworldly jungle at twilight, with colossal, crystalline trees reflecting a nebula-filled sky. In the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. The overall atmosphere should be one of serene magic and vibrant detail.\n",
    "\"\"\"\n",
    "negative_prompt = \"cartoon, anime, poor quality, poor clarity, ugly, jpeg artifacts, cropped, lowres, error, out of frame, watermark\"\n",
    "\n",
    "# Global parameters\n",
    "width, height = 1536, 640\n",
    "system_prompt = \"You are an art expert with emphasis in whimsical photorealistic creations, please convey intricate details with the highest degree of aesthetics: \"\n",
    "\n",
    "# Model-specific parameters - customize as needed\n",
    "models = {\n",
    "    \"cog4\": {\"id\": \"THUDM/CogView4-6B\", \"qty\": 3, \"base_cfg\": 4, \"cfg_step\": 0.5, \"base_steps\": 25, \"step_incr\": 5},\n",
    "    \"lumina2\": {\"id\": \"Alpha-VLLM/Lumina-Image-2.0\", \"qty\": 2, \"base_cfg\": 4.5, \"cfg_step\": 0.5, \"base_steps\": 25, \"step_incr\": 5, \"cfg_trunc\": 1.1, \"cfg_norm\": True},\n",
    "    \"aura3\": {\"id\": \"fal/AuraFlow-v0.3\", \"qty\": 2, \"base_cfg\": 4.5, \"cfg_step\": 0.5, \"base_steps\": 25, \"step_incr\": 5},\n",
    "    \"sd35_med\": {\"id\": \"stabilityai/stable-diffusion-3.5-medium\", \"qty\": 4, \"base_cfg\": 5, \"cfg_step\": 0.7, \"base_steps\": 30, \"step_incr\": 5},\n",
    "    \"kolors\": {\"id\": \"Kwai-Kolors/Kolors-diffusers\", \"qty\": 6, \"base_cfg\": 3.5, \"cfg_step\": 0.5, \"base_steps\": 25, \"step_incr\": 5},\n",
    "    \"sd35_large\": {\"id\": \"stabilityai/stable-diffusion-3.5-large\", \"qty\": 3, \"base_cfg\": 4.5, \"cfg_step\": 0.7, \"base_steps\": 20, \"step_incr\": 5}\n",
    "}\n",
    "\n",
    "import diffusers, torch, time, gc, os, subprocess, math, json\n",
    "from datetime import datetime\n",
    "\n",
    "def flush(): gc.collect(); torch.cuda.empty_cache()\n",
    "def bytes_to_giga_bytes(bytes): return bytes / 1024 / 1024 / 1024\n",
    "device, dtype = \"cuda\", torch.bfloat16\n",
    "\n",
    "# Stats collection\n",
    "stats = {\"start_time\": time.time(), \"models\": {}, \"total_images\": 0, \"total_vram_gb\": 0, \"total_gen_time\": 0}\n",
    "for model in models: stats[\"models\"][model] = {\"gen_times\": [], \"vram_usage\": [], \"images\": []}\n",
    "\n",
    "# Run all models and collect stats\n",
    "def run_all_models():\n",
    "    # CogView4\n",
    "    run_cog4()\n",
    "    \n",
    "    # Lumina2\n",
    "    run_lumina2()\n",
    "    \n",
    "    # AuraFlow\n",
    "    run_aura3()\n",
    "    \n",
    "    # SD 3.5 Medium\n",
    "    run_sd35_medium()\n",
    "    \n",
    "    # Kolors\n",
    "    run_kolors()\n",
    "    \n",
    "    # SD 3.5 Large\n",
    "    run_sd35_large()\n",
    "    \n",
    "    # Print summary\n",
    "    print_summary()\n",
    "\n",
    "def run_cog4():\n",
    "    model_key = \"cog4\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    emb_prompts = diffusers.DiffusionPipeline.from_pretrained(model_id, transformer=None, vae=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (prompt_embeds, negative_prompt_embeds) = emb_prompts.encode_prompt(prompt=prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "    del emb_prompts\n",
    "    flush()\n",
    "    print(f\"   ... {model_key} Prompts embedded.. {time.time() - time_start:.2f} seconds, Max vram: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB\\n   ... Generating {model_cfg['qty']} Images..\")\n",
    "\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, text_encoder=None, tokenizer=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]): \n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "        \n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(prompt_embeds=prompt_embeds.to(device).to(dtype), negative_prompt_embeds=negative_prompt_embeds.to(device).to(dtype), guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "            \n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "\n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def run_lumina2():\n",
    "    model_key = \"lumina2\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_gen = time.time()\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]): \n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "\n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(system_prompt=system_prompt, prompt=prompt, negative_prompt=negative_prompt, guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height, cfg_trunc_ratio=model_cfg[\"cfg_trunc\"], cfg_normalization=model_cfg[\"cfg_norm\"]).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "\n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "\n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def run_aura3():\n",
    "    model_key = \"aura3\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_gen = time.time()\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]): \n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "        \n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(prompt=prompt, negative_prompt=negative_prompt, guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "            \n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "    \n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def run_sd35_medium():\n",
    "    model_key = \"sd35_med\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_gen = time.time()\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]):\n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "\n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(prompt=prompt, prompt_2=prompt, prompt_3=prompt, negative_prompt=negative_prompt, guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "\n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "\n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def run_kolors():\n",
    "    model_key = \"kolors\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    emb_prompts = diffusers.DiffusionPipeline.from_pretrained(model_id, unet=None, vae=None, variant=\"fp16\", torch_dtype=dtype).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds,) = emb_prompts.encode_prompt(prompt=prompt, negative_prompt=negative_prompt)\n",
    "    del emb_prompts\n",
    "    flush()\n",
    "    \n",
    "    print(f\"   ... {model_key} Prompts embedded.. {time.time() - time_start:.2f} seconds, Max vram: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB\\n   ... Generating {model_cfg['qty']} Images..\")\n",
    "\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, text_encoder=None, tokenizer=None, variant=\"fp16\", torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]):\n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "        \n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(prompt_embeds=prompt_embeds.to(device).to(dtype), negative_prompt_embeds=negative_prompt_embeds.to(device).to(dtype), pooled_prompt_embeds=pooled_prompt_embeds.to(device).to(dtype), negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device).to(dtype), guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "            \n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "        \n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def run_sd35_large():\n",
    "    model_key = \"sd35_large\"\n",
    "    model_cfg = models[model_key]\n",
    "    model_id = model_cfg[\"id\"]\n",
    "    \n",
    "    time_start = time.time()\n",
    "    emb_prompts = diffusers.DiffusionPipeline.from_pretrained(model_id, transformer=None, vae=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        (prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds,) = emb_prompts.encode_prompt(prompt=prompt, prompt_2=prompt, prompt_3=prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "    del emb_prompts\n",
    "    flush()\n",
    "    \n",
    "    print(f\"   ... {model_key} Prompts embedded.. {time.time() - time_start:.2f} seconds, Max vram: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB\\n   ... Generating {model_cfg['qty']} Images..\")\n",
    "\n",
    "    pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, text_encoder=None, text_encoder_2=None, text_encoder_3=None, tokenizer=None, tokenizer_2=None, tokenizer_3=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "    for i in range(model_cfg[\"qty\"]):\n",
    "        g_scale = model_cfg[\"base_cfg\"] + i * model_cfg[\"cfg_step\"]\n",
    "        steps = model_cfg[\"base_steps\"] + i * model_cfg[\"step_incr\"]\n",
    "        \n",
    "        time_gen_start = time.time()\n",
    "        with torch.inference_mode():\n",
    "            image = pipeline(prompt_embeds=prompt_embeds.to(device).to(dtype), negative_prompt_embeds=negative_prompt_embeds.to(device).to(dtype), pooled_prompt_embeds=pooled_prompt_embeds.to(device).to(dtype), negative_pooled_prompt_embeds=negative_pooled_prompt_embeds.to(device).to(dtype), guidance_scale=g_scale, num_inference_steps=steps, width=width, height=height).images[0]\n",
    "        gen_time = time.time() - time_gen_start\n",
    "            \n",
    "        filename = f\"{model_key}_cfg_{g_scale:.1f}_steps_{int(steps)}_{str(int(time.time()))}.png\"\n",
    "        result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "        image.save(filename)\n",
    "        os.startfile(filename)\n",
    "        \n",
    "        vram_used = bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n",
    "        print(f\"   ... Generated in {gen_time:.2f} secs, mem/temp/use: {result.strip()}   ... Max mem: {vram_used:.2f} GB\")\n",
    "        \n",
    "        # Save stats\n",
    "        stats[\"models\"][model_key][\"gen_times\"].append(gen_time)\n",
    "        stats[\"models\"][model_key][\"vram_usage\"].append(vram_used)\n",
    "        stats[\"models\"][model_key][\"images\"].append({\"filename\": filename, \"cfg\": g_scale, \"steps\": steps, \"time\": gen_time})\n",
    "        stats[\"total_images\"] += 1\n",
    "        stats[\"total_gen_time\"] += gen_time\n",
    "        stats[\"total_vram_gb\"] = max(stats[\"total_vram_gb\"], vram_used)\n",
    "        \n",
    "    del pipeline\n",
    "    flush()\n",
    "\n",
    "def print_summary():\n",
    "    total_time = time.time() - stats[\"start_time\"]\n",
    "    avg_image_size_kb = 2000 \n",
    "    image_bytes = (avg_image_size_kb * 1024) * stats[\"total_images\"]\n",
    "    vram_bytes = stats[\"total_vram_gb\"] * 1024 * 1024 * 1024\n",
    "    \n",
    "    # Calculate model averages\n",
    "    model_avgs = {}\n",
    "    fastest_model = {\"name\": \"\", \"time\": float('inf')}\n",
    "    slowest_model = {\"name\": \"\", \"time\": 0}\n",
    "    most_efficient = {\"name\": \"\", \"ratio\": 0}  # Time per step\n",
    "\n",
    "    for model_name, model_data in stats[\"models\"].items():\n",
    "        if not model_data[\"gen_times\"]: continue\n",
    "        avg_time = sum(model_data[\"gen_times\"]) / len(model_data[\"gen_times\"])\n",
    "        avg_vram = sum(model_data[\"vram_usage\"]) / len(model_data[\"vram_usage\"])\n",
    "        avg_steps = sum(image[\"steps\"] for image in model_data[\"images\"]) / len(model_data[\"images\"])\n",
    "        time_per_step = avg_time / avg_steps\n",
    "        \n",
    "        model_avgs[model_name] = {\n",
    "            \"avg_time\": avg_time,\n",
    "            \"avg_vram\": avg_vram,\n",
    "            \"time_per_step\": time_per_step\n",
    "        }\n",
    "        \n",
    "        # Track fastest/slowest\n",
    "        if avg_time < fastest_model[\"time\"]:\n",
    "            fastest_model = {\"name\": model_name, \"time\": avg_time}\n",
    "        if avg_time > slowest_model[\"time\"]:\n",
    "            slowest_model = {\"name\": model_name, \"time\": avg_time}\n",
    "        \n",
    "        # Track efficiency (lower is better)\n",
    "        efficiency_ratio = time_per_step\n",
    "        if most_efficient[\"name\"] == \"\" or efficiency_ratio < most_efficient[\"ratio\"]:\n",
    "            most_efficient = {\"name\": model_name, \"ratio\": efficiency_ratio}\n",
    "    \n",
    "    # Fun data comparisons\n",
    "    fun_facts = [\n",
    "        f\"The total raw image data generated ({image_bytes/1024/1024:.1f} MB) is equivalent to approximately {image_bytes/150000:.1f} floppy disks from the 1990s.\",\n",
    "        f\"Peak VRAM usage of {stats['total_vram_gb']:.2f} GB is approximately {stats['total_vram_gb']/0.000001:.0f}x the total RAM available in the first IBM PC (1981).\",\n",
    "        f\"You generated {stats['total_images']} images in {total_time:.1f} seconds - that's {stats['total_images']/(total_time/60):.1f} images per minute!\",\n",
    "        f\"The computational power used would have required a supercomputer the size of a room in the early 2000s.\",\n",
    "        f\"In 1995, this generation would have taken approximately {total_time * 1000:.0f} hours on high-end hardware.\"\n",
    "    ]\n",
    "    \n",
    "    # Print summary report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"ðŸ“Š IMAGE GENERATION BENCHMARK SUMMARY - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ðŸ–¼ï¸  Total Images: {stats['total_images']} images across {len(stats['models'])} models\")\n",
    "    print(f\"â±ï¸  Total Runtime: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "    print(f\"ðŸ’¾  Peak VRAM Usage: {stats['total_vram_gb']:.2f} GB\")\n",
    "    print(f\"âš¡  Avg Generation Time: {stats['total_gen_time']/stats['total_images']:.2f} seconds per image\")\n",
    "    print(f\"ðŸš€  Fastest Model: {fastest_model['name']} ({fastest_model['time']:.2f} seconds avg)\")\n",
    "    print(f\"ðŸ¢  Slowest Model: {slowest_model['name']} ({slowest_model['time']:.2f} seconds avg)\")\n",
    "    print(f\"âœ¨  Most Efficient: {most_efficient['name']} ({most_efficient['ratio']:.4f} sec/step)\")\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ MODEL COMPARISON:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Model':12} | {'Avg Time (s)':12} | {'Avg VRAM (GB)':14} | {'Time/Step (s)':12}\")\n",
    "    print(\"-\"*80)\n",
    "    for model_name, data in model_avgs.items():\n",
    "        print(f\"{model_name:12} | {data['avg_time']:12.2f} | {data['avg_vram']:14.2f} | {data['time_per_step']:12.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸŽ® FUN FACTS:\")\n",
    "    for fact in fun_facts:\n",
    "        print(f\"  â€¢ {fact}\")\n",
    "    \n",
    "    print(\"\\nðŸ“ PROMPT:\")\n",
    "    print(f\"  \\\"{prompt[:100]}{'...' if len(prompt) > 100 else ''}\\\"\")\n",
    "    \n",
    "    # Save stats to file\n",
    "    with open(f\"benchmark_stats_{int(time.time())}.json\", \"w\") as f:\n",
    "        json.dump(stats, f)\n",
    "    \n",
    "    print(\"\\nâœ… Complete! Stats saved to benchmark_stats_[timestamp].json\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Execute all model runs\n",
    "if __name__ == \"__main__\":\n",
    "    run_all_models()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380d5a7-46f1-466e-a3e4-dd7f493b0ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
