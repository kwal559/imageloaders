{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4c1e5-1c0e-4828-83fa-9ef627b20d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/diffusers  \n",
    "#credit to ASOMOZA..  additions are neg prompt embeddings, only the full repo, allowing for guidance manipulation.. and even if it wont make it to video memory, still faster then the chopped up text encoders + q5 gguf i tried ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baab21c-f296-4c7e-8504-67d69e3b21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time, os, subprocess\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "from diffusers import AutoencoderKL, HiDreamImagePipeline, HiDreamImageTransformer2DModel, UniPCMultistepScheduler\n",
    "from diffusers.hooks.group_offloading import apply_group_offloading\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "\n",
    "repo_id = \"HiDream-ai/HiDream-I1-Full\"\n",
    "llama_repo_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch_dtype = torch.bfloat16\n",
    "\n",
    "prompt = \"\"\"\n",
    "Candid photo of a dark space fantasy cenobite with nails partially driven into its head, he is sitting at a table inside a 1970s tavern, there are 2 diverse, beautiful human females with serpents for hair, they slither anid hiss. dimly lit rustic atmosphere, other humans stare nervously at this nightmare.\n",
    "\n",
    "\"\"\"\n",
    "negative_prompt = \"cartoon, anime\"\n",
    "\n",
    "width=1344\n",
    "height=768\n",
    "guidance_scale=4.5\n",
    "num_inference_steps=35\n",
    "\n",
    "def flush(device):\n",
    "    \"\"\"Helper function to clear CUDA memory.\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "\n",
    "\n",
    "# Corrected encode_prompt to manage memory, REVERT Llama embedding to 4D,\n",
    "# ensure dtype consistency, and use correct dictionary keys for pooled embeddings.\n",
    "def encode_prompt(\n",
    "    prompt, negative_prompt, pipeline_repo_id, llama_repo_id, device=device, dtype=torch_dtype\n",
    "):\n",
    "    print(\"Generating embeddings...\")\n",
    "    # Ensure prompts are lists\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    # Ensure negative prompt list matches prompt list size\n",
    "    negative_prompt = [negative_prompt] * len(prompt)\n",
    "\n",
    "    # --- CLIP 1 ---\n",
    "    print(\"  Loading CLIP 1...\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer\")\n",
    "    # Load to CPU first, then move to device and cast dtype explicitly\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder\"\n",
    "    )\n",
    "    text_encoder = text_encoder.to(device=device, dtype=dtype) # Explicitly cast\n",
    "\n",
    "    prompt_embeds = get_clip_prompt_embeds(prompt, tokenizer, text_encoder)\n",
    "    prompt_embeds_1 = prompt_embeds.clone().detach()\n",
    "    negative_prompt_embeds = get_clip_prompt_embeds(negative_prompt, tokenizer, text_encoder)\n",
    "    negative_prompt_embeds_1 = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    print(\"  Unloading CLIP 1...\")\n",
    "    text_encoder.to(\"cpu\") # Move back to CPU to free VRAM\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del tokenizer\n",
    "    del text_encoder\n",
    "    flush(device)\n",
    "\n",
    "    # --- CLIP 2 ---\n",
    "    print(\"  Loading CLIP 2...\")\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer_2\")\n",
    "    # Load to CPU first, then move to device and cast dtype explicitly\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder_2\"\n",
    "    )\n",
    "    text_encoder = text_encoder.to(device=device, dtype=dtype) # Explicitly cast\n",
    "\n",
    "    prompt_embeds = get_clip_prompt_embeds(prompt, tokenizer, text_encoder)\n",
    "    prompt_embeds_2 = prompt_embeds.clone().detach()\n",
    "    negative_prompt_embeds = get_clip_prompt_embeds(negative_prompt, tokenizer, text_encoder)\n",
    "    negative_prompt_embeds_2 = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    print(\"  Unloading CLIP 2...\")\n",
    "    text_encoder.to(\"cpu\") # Move back to CPU to free VRAM\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del tokenizer\n",
    "    del text_encoder\n",
    "    flush(device)\n",
    "\n",
    "    # --- Pooled Embeddings ---\n",
    "    # Concatenate the output[0] (projected embeddings) from CLIP 1 and CLIP 2\n",
    "    pooled_prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)\n",
    "    negative_pooled_prompt_embeds = torch.cat([negative_prompt_embeds_1, negative_prompt_embeds_2], dim=-1)\n",
    "    del prompt_embeds_1, prompt_embeds_2, negative_prompt_embeds_1, negative_prompt_embeds_2 # Clean up intermediate CLIP embeddings\n",
    "\n",
    "    # --- T5 ---\n",
    "    print(\"  Loading T5...\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer_3\")\n",
    "    # Load to CPU first, then move to device and cast dtype explicitly\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder_3\"\n",
    "    )\n",
    "    text_encoder = text_encoder.to(device=device, dtype=dtype) # Explicitly cast\n",
    "\n",
    "\n",
    "    # Positive T5\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids.to(device)\n",
    "    attention_mask = text_inputs.attention_mask.to(device) # Ensure attention mask is also on device\n",
    "    # T5 output [0] is the last hidden state\n",
    "    prompt_embeds = text_encoder(text_input_ids, attention_mask=attention_mask)[0]\n",
    "    t5_prompt_embeds = prompt_embeds.clone().detach()\n",
    "\n",
    "    # Negative T5\n",
    "    negative_text_inputs = tokenizer(\n",
    "        negative_prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        )\n",
    "    negative_input_ids = negative_text_inputs.input_ids.to(device)\n",
    "    negative_attention_mask = negative_text_inputs.attention_mask.to(device) # Ensure attention mask is also on device\n",
    "    negative_prompt_embeds = text_encoder(negative_input_ids, attention_mask=negative_attention_mask)[0]\n",
    "    t5_negative_prompt_embeds = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    print(\"  Unloading T5...\")\n",
    "    text_encoder.to(\"cpu\") # Move back to CPU to free VRAM\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del text_inputs, negative_text_inputs, attention_mask, negative_attention_mask\n",
    "    del text_encoder\n",
    "    del tokenizer\n",
    "    flush(device)\n",
    "\n",
    "    # --- Llama3 (Text Encoder 4) ---\n",
    "    print(\"  Loading Llama3...\")\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(llama_repo_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token # Ensure pad token is set\n",
    "    # Load to CPU first, then move to device and cast dtype explicitly\n",
    "    text_encoder = LlamaForCausalLM.from_pretrained(\n",
    "        llama_repo_id,\n",
    "        output_hidden_states=True, # Still need this to access hidden states\n",
    "        output_attentions=False, # Don't need attentions, saves memory during forward pass\n",
    "    )\n",
    "    text_encoder = text_encoder.to(device=device, dtype=dtype) # Explicitly cast\n",
    "\n",
    "    # Positive Llama3\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids.to(device)\n",
    "    attention_mask = text_inputs.attention_mask.to(device) # Ensure attention mask is also on device\n",
    "    outputs = text_encoder(\n",
    "        text_input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        output_hidden_states=True, # Request hidden states\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    # *** REVERTED CHANGE: Stack hidden states from layer 1 onwards to get 4D tensor ***\n",
    "    # This produces shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "    llama3_prompt_embeds = torch.stack(outputs.hidden_states[1:], dim=0).clone().detach()\n",
    "\n",
    "\n",
    "    # Negative Llama3\n",
    "    negative_text_inputs = tokenizer(\n",
    "        negative_prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    negative_input_ids = negative_text_inputs.input_ids.to(device)\n",
    "    negative_attention_mask = negative_text_inputs.attention_mask.to(device) # Ensure attention mask is also on device\n",
    "    outputs = text_encoder(\n",
    "        negative_input_ids,\n",
    "        attention_mask=negative_attention_mask,\n",
    "        output_hidden_states=True, # Request hidden states\n",
    "        output_attentions=False,\n",
    "    )\n",
    "    # *** REVERTED CHANGE: Stack hidden states from layer 1 onwards to get 4D tensor ***\n",
    "    # This produces shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "    llama3_negative_prompt_embeds = torch.stack(outputs.hidden_states[1:], dim=0).clone().detach()\n",
    "\n",
    "\n",
    "    print(\"  Unloading Llama3...\")\n",
    "    text_encoder.to(\"cpu\") # Move back to CPU to free VRAM\n",
    "    del outputs, text_inputs, negative_text_inputs, attention_mask, negative_attention_mask, text_encoder, tokenizer\n",
    "    flush(device)\n",
    "\n",
    "    # --- Assemble Embeddings ---\n",
    "    # Use the correct keys expected by the pipeline's __call__ method when passing pre-computed embeddings\n",
    "    embeds = {\n",
    "        \"prompt_embeds_t5\": t5_prompt_embeds,\n",
    "        \"prompt_embeds_llama3\": llama3_prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"negative_prompt_embeds_t5\": t5_negative_prompt_embeds,\n",
    "        \"negative_prompt_embeds_llama3\": llama3_negative_prompt_embeds,\n",
    "        \"negative_pooled_prompt_embeds\": negative_pooled_prompt_embeds,\n",
    "    }\n",
    "    print(\"Embeddings generated.\")\n",
    "\n",
    "    return embeds\n",
    "\n",
    "\n",
    "def get_clip_prompt_embeds(prompt, tokenizer, text_encoder):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    # Ensure input IDs are on the device and are Long type\n",
    "    text_input_ids = text_inputs.input_ids.to(device, dtype=torch.long)\n",
    "\n",
    "    # Pass input IDs to the text encoder.\n",
    "    # The encoder is expected to be already on the correct device and dtype\n",
    "    # due to the explicit .to(device=device, dtype=dtype) call in encode_prompt.\n",
    "    outputs = text_encoder(text_input_ids, output_hidden_states=True)\n",
    "\n",
    "    # CLIPTextModelWithProjection output is typically a tuple: (text_embeds, pooled_output, hidden_states...)\n",
    "    # text_embeds is the result of text_projection(pooled_output)\n",
    "    # We take outputs[0], the final projected embedding as in your original code.\n",
    "    prompt_embeds = outputs[0]\n",
    "\n",
    "    return prompt_embeds\n",
    "\n",
    "\n",
    "def denoise(embeddings, device=device, dtype=torch_dtype):\n",
    "    print(\"Loading Denoising components...\")\n",
    "    scheduler = UniPCMultistepScheduler(\n",
    "        flow_shift=3.0,\n",
    "        prediction_type=\"flow_prediction\",\n",
    "        use_flow_sigmas=True,\n",
    "    )\n",
    "\n",
    "    transformer = HiDreamImageTransformer2DModel.from_pretrained(\n",
    "        \"HiDream-ai/HiDream-I1-Full\", subfolder=\"transformer\", torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    # Apply offloading to the transformer\n",
    "    transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch_dtype)\n",
    "    apply_group_offloading(\n",
    "        transformer,\n",
    "        onload_device=device,\n",
    "        offload_device=torch.device(\"cpu\"),\n",
    "        offload_type=\"leaf_level\",\n",
    "        use_stream=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    # Load VAE only for scaling factor within the pipeline object initialization\n",
    "    # It will be loaded again later for actual decoding outside the pipeline call.\n",
    "    # Load to CPU initially, it seems the pipeline constructor might not need it on device immediately\n",
    "    vae_init = AutoencoderKL.from_pretrained(repo_id, subfolder=\"vae\", torch_dtype=torch_dtype)\n",
    "\n",
    "\n",
    "    print(\"Instantiating pipeline (without text encoders)...\")\n",
    "    # Instantiate the pipeline by passing only the components it needs for the diffusion process\n",
    "    # and which will receive the pre-computed embeddings.\n",
    "    # We explicitly pass None for text encoders and tokenizers as we're using pre-computed embeddings.\n",
    "    # REMOVED torch_dtype=torch_dtype from THIS constructor call - this was the fix for the last error\n",
    "    pipe = HiDreamImagePipeline(\n",
    "        vae=vae_init, # VAE is needed for scaling\n",
    "        transformer=transformer,\n",
    "        scheduler=scheduler,\n",
    "        text_encoder=None,\n",
    "        tokenizer=None,\n",
    "        text_encoder_2=None,\n",
    "        tokenizer_2=None,\n",
    "        text_encoder_3=None,\n",
    "        tokenizer_3=None,\n",
    "        text_encoder_4=None, # Ensure Text Encoder 4 (Llama) is also None\n",
    "        tokenizer_4=None,\n",
    "        # Removed: torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    # Move pipeline components (transformer, scheduler, and potentially VAE_init) to device\n",
    "    # Note: The Transformer will handle its own offloading due to apply_group_offloading\n",
    "    pipe.to(device)\n",
    "\n",
    "    print(\"Running pipeline...\")\n",
    "    # Pass the pre-computed embeddings to the pipeline\n",
    "    latents = pipe(\n",
    "        prompt_embeds_t5=embeddings[\"prompt_embeds_t5\"],\n",
    "        prompt_embeds_llama3=embeddings[\"prompt_embeds_llama3\"],\n",
    "        pooled_prompt_embeds=embeddings[\"pooled_prompt_embeds\"],\n",
    "        negative_prompt_embeds_t5=embeddings[\"negative_prompt_embeds_t5\"],\n",
    "        negative_prompt_embeds_llama3=embeddings[\"negative_prompt_embeds_llama3\"],\n",
    "        negative_pooled_prompt_embeds=embeddings[\"negative_pooled_prompt_embeds\"],\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=torch.Generator(device).manual_seed(0), # Generator on device\n",
    "        output_type=\"latent\", # Output latents as in your original code\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "    print(\"Denoising complete.\")\n",
    "    # Clean up pipeline components if necessary (though they might offload themselves)\n",
    "    del pipe, transformer, scheduler, vae_init\n",
    "    flush(device)\n",
    "\n",
    "    return latents\n",
    "\n",
    "# --- Main Execution ---\n",
    "time_gen = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Generate embeddings using the memory-managed function\n",
    "    embeddings = encode_prompt(prompt, negative_prompt, repo_id, llama_repo_id, device=device, dtype=torch_dtype)\n",
    "\n",
    "\n",
    "# Denoise using the pre-computed latents\n",
    "latents = denoise(embeddings, device=device, dtype=torch_dtype)\n",
    "\n",
    "print(\"Loading VAE for decoding...\")\n",
    "# Load VAE separately for decoding, as denoise outputs latents\n",
    "vae = AutoencoderKL.from_pretrained(repo_id, subfolder=\"vae\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "# Apply scaling factor and shift factor before decoding\n",
    "latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Decoding latents...\")\n",
    "    image = vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "print(\"Post-processing image...\")\n",
    "# The HiDream VAE scaling factor for the image processor is typically vae.config.scaling_factor * 8\n",
    "# based on the model architecture (latent size is image_size / 8).\n",
    "# The correct calculation is vae.config.scaling_factor * 2^(num_downsampling_blocks)\n",
    "vae_scale_factor = vae.config.scaling_factor * (2 ** (len(vae.config.block_out_channels) - 1))\n",
    "image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor) # Use the derived scale factor\n",
    "image = image_processor.postprocess(image, output_type=\"pil\")[0]\n",
    "\n",
    "\n",
    "# Clean up VAE after decoding\n",
    "del vae\n",
    "flush(device)\n",
    "\n",
    "\n",
    "filename = f\"hidream_cfg{guidance_scale}_steps_{num_inference_steps}_{str(int(time.time()))}.png\"\n",
    "\n",
    "print(\"\\n--- NVIDIA-SMI Stats ---\")\n",
    "try:\n",
    "    result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=10.0)\n",
    "    print(result.strip())\n",
    "except FileNotFoundError:\n",
    "    print(\"nvidia-smi not found. Cannot retrieve GPU stats.\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"nvidia-smi timed out.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error running nvidia-smi: {e}\")\n",
    "print(\"----------------------\\n\")\n",
    "\n",
    "image.save(filename)\n",
    "print(f\"Generated image saved as {filename}\")\n",
    "os.startfile(filename)\n",
    "print(f\"   ... Total generation time: {time.time() - time_gen:.2f} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29484cb9-7fb7-44b8-a388-4f53c0700de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
