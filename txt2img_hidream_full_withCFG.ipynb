{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e4c1e5-1c0e-4828-83fa-9ef627b20d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/huggingface/diffusers  \n",
    "#credit to ASOMOZA..  additions are neg prompt embeddings, only the full repo, allowing for guidance manipulation.. and even if it wont make it to video memory, still faster then the chopped up text encoders + q5 gguf i tried ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baab21c-f296-4c7e-8504-67d69e3b21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, time, os, subprocess\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    LlamaForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    T5EncoderModel,\n",
    "    T5Tokenizer,\n",
    ")\n",
    "\n",
    "from diffusers import AutoencoderKL, HiDreamImagePipeline, HiDreamImageTransformer2DModel, UniPCMultistepScheduler\n",
    "from diffusers.hooks.group_offloading import apply_group_offloading\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "\n",
    "repo_id = \"HiDream-ai/HiDream-I1-Full\"\n",
    "llama_repo_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "device = torch.device(\"cuda\")\n",
    "torch_dtype = torch.bfloat16\n",
    "prompt = \"A photorealistic close-up of a single, iridescent hummingbird hovering mid-air, its wings a blur of sapphire and emerald, drinking nectar from a bioluminescent flower that emits soft, swirling particles of golden light. In the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. The background is a hyper-detailed, otherworldly jungle at twilight, with colossal, natural crystalline trees reflecting a nebula-filled sky. The overall atmosphere should be one of serene magic and vibrant detail\"\n",
    "\n",
    "negative_prompt = \"cartoon, anime\"\n",
    "\n",
    "width=1344\n",
    "height=768\n",
    "guidance_scale=7.5\n",
    "num_inference_steps=28\n",
    "\n",
    "def flush(device):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "    print(f\"Current CUDA memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "    print(f\"Current CUDA memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "\n",
    "\n",
    "# Modified encode_prompt to include negative prompt handling\n",
    "def encode_prompt(\n",
    "    prompt, negative_prompt, pipeline_repo_id, llama_repo_id, device=device, dtype=torch_dtype\n",
    "):\n",
    "    # Ensure prompts are lists\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    negative_prompt = [negative_prompt] * len(prompt) # Create list from single negative prompt\n",
    "\n",
    "    # --- CLIP 1 ---\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer\")\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder\", torch_dtype=torch_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    prompt_embeds = get_clip_prompt_embeds(prompt, tokenizer, text_encoder)\n",
    "    prompt_embeds_1 = prompt_embeds.clone().detach()\n",
    "    negative_prompt_embeds = get_clip_prompt_embeds(negative_prompt, tokenizer, text_encoder)\n",
    "    negative_prompt_embeds_1 = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    text_encoder.to(\"cpu\")\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del tokenizer\n",
    "    del text_encoder\n",
    "    flush(device)\n",
    "\n",
    "    # --- CLIP 2 ---\n",
    "    tokenizer = CLIPTokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer_2\")\n",
    "    text_encoder = CLIPTextModelWithProjection.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder_2\", torch_dtype=torch_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    prompt_embeds = get_clip_prompt_embeds(prompt, tokenizer, text_encoder)\n",
    "    prompt_embeds_2 = prompt_embeds.clone().detach()\n",
    "    negative_prompt_embeds = get_clip_prompt_embeds(negative_prompt, tokenizer, text_encoder)\n",
    "    negative_prompt_embeds_2 = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    text_encoder.to(\"cpu\")\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del tokenizer\n",
    "    del text_encoder\n",
    "    flush(device)\n",
    "\n",
    "    # --- Pooled Embeddings ---\n",
    "    pooled_prompt_embeds = torch.cat([prompt_embeds_1, prompt_embeds_2], dim=-1)\n",
    "    negative_pooled_prompt_embeds = torch.cat([negative_prompt_embeds_1, negative_prompt_embeds_2], dim=-1)\n",
    "\n",
    "    # --- T5 ---\n",
    "    tokenizer = T5Tokenizer.from_pretrained(pipeline_repo_id, subfolder=\"tokenizer_3\", torch_dtype=torch_dtype)\n",
    "    text_encoder = T5EncoderModel.from_pretrained(\n",
    "        pipeline_repo_id, subfolder=\"text_encoder_3\", torch_dtype=torch_dtype\n",
    "    ).to(device)\n",
    "\n",
    "    # Positive T5\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    attention_mask = text_inputs.attention_mask\n",
    "    prompt_embeds = text_encoder(text_input_ids.to(device), attention_mask=attention_mask.to(device))[0]\n",
    "    t5_prompt_embeds = prompt_embeds.clone().detach()\n",
    "\n",
    "    # Negative T5\n",
    "    negative_text_inputs = tokenizer(\n",
    "        negative_prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "        )\n",
    "    negative_input_ids = negative_text_inputs.input_ids\n",
    "    negative_attention_mask = negative_text_inputs.attention_mask\n",
    "    negative_prompt_embeds = text_encoder(negative_input_ids.to(device), attention_mask=negative_attention_mask.to(device))[0]\n",
    "    t5_negative_prompt_embeds = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del text_inputs, negative_text_inputs\n",
    "    del text_encoder\n",
    "    del tokenizer\n",
    "    flush(device)\n",
    "\n",
    "    # --- Llama3 ---\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(llama_repo_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    text_encoder = LlamaForCausalLM.from_pretrained(\n",
    "        llama_repo_id,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True, # Keep original settings\n",
    "        torch_dtype=torch_dtype,\n",
    "    ).to(device)\n",
    "\n",
    "    # Positive Llama3\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    attention_mask = text_inputs.attention_mask\n",
    "    outputs = text_encoder(\n",
    "        text_input_ids.to(device),\n",
    "        attention_mask=attention_mask.to(device),\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True, \n",
    "    )\n",
    "    # Stack all hidden layers (excluding input embeddings layer 0)\n",
    "    prompt_embeds = torch.stack(outputs.hidden_states[1:], dim=0)\n",
    "    llama3_prompt_embeds = prompt_embeds.clone().detach()\n",
    "\n",
    "    # Negative Llama3\n",
    "    negative_text_inputs = tokenizer(\n",
    "        negative_prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    negative_input_ids = negative_text_inputs.input_ids\n",
    "    negative_attention_mask = negative_text_inputs.attention_mask\n",
    "    outputs = text_encoder(\n",
    "        negative_input_ids.to(device),\n",
    "        attention_mask=negative_attention_mask.to(device),\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "    )\n",
    "    # Stack all hidden layers (excluding input embeddings layer 0)\n",
    "    negative_prompt_embeds = torch.stack(outputs.hidden_states[1:], dim=0)\n",
    "    llama3_negative_prompt_embeds = negative_prompt_embeds.clone().detach()\n",
    "\n",
    "    del prompt_embeds, negative_prompt_embeds\n",
    "    del outputs\n",
    "    del text_inputs, negative_text_inputs\n",
    "    del text_encoder\n",
    "    del tokenizer\n",
    "    flush(device)\n",
    "\n",
    "    # --- Assemble Embeddings ---\n",
    "    final_prompt_embeds = [t5_prompt_embeds, llama3_prompt_embeds]\n",
    "    final_negative_prompt_embeds = [t5_negative_prompt_embeds, llama3_negative_prompt_embeds]\n",
    "\n",
    "    embeds = {\n",
    "        \"prompt_embeds\": final_prompt_embeds,\n",
    "        \"pooled_prompt_embeds\": pooled_prompt_embeds,\n",
    "        \"negative_prompt_embeds\": final_negative_prompt_embeds,\n",
    "        \"negative_pooled_prompt_embeds\": negative_pooled_prompt_embeds,\n",
    "    }\n",
    "\n",
    "    return embeds\n",
    "\n",
    "\n",
    "\n",
    "def get_clip_prompt_embeds(prompt, tokenizer, text_encoder):\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=77,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids.to(device) \n",
    "    prompt_embeds = text_encoder(text_input_ids, output_hidden_states=True)\n",
    "\n",
    "    \n",
    "    prompt_embeds = prompt_embeds[0]\n",
    "\n",
    "    return prompt_embeds\n",
    "\n",
    "\n",
    "def denoise(embeddings, device=device, dtype=torch_dtype):\n",
    "    scheduler = UniPCMultistepScheduler(\n",
    "        flow_shift=3.0,\n",
    "        prediction_type=\"flow_prediction\",\n",
    "        use_flow_sigmas=True,\n",
    "    )\n",
    "\n",
    "    transformer = HiDreamImageTransformer2DModel.from_pretrained(\n",
    "        \"HiDream-ai/HiDream-I1-Full\", subfolder=\"transformer\", torch_dtype=torch_dtype\n",
    "    )\n",
    "\n",
    "    transformer.enable_layerwise_casting(storage_dtype=torch.float8_e4m3fn, compute_dtype=torch_dtype)\n",
    "    apply_group_offloading(\n",
    "        transformer,\n",
    "        onload_device=device,\n",
    "        offload_device=torch.device(\"cpu\"),\n",
    "        offload_type=\"leaf_level\",\n",
    "        use_stream=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    pipe = HiDreamImagePipeline.from_pretrained(\n",
    "        repo_id,\n",
    "        text_encoder=None,\n",
    "        tokenizer=None,\n",
    "        text_encoder_2=None,\n",
    "        tokenizer_2=None,\n",
    "        text_encoder_3=None,\n",
    "        tokenizer_3=None,\n",
    "        text_encoder_4=None,\n",
    "        tokenizer_4=None,\n",
    "        transformer=transformer,\n",
    "        scheduler=scheduler,\n",
    "        vae=None,\n",
    "        torch_dtype=torch_dtype,\n",
    "    )\n",
    "\n",
    "    \n",
    "    prompt_embeds_t5, prompt_embeds_llama3 = embeddings[\"prompt_embeds\"]\n",
    "    pooled_prompt_embeds = embeddings[\"pooled_prompt_embeds\"]\n",
    "    negative_prompt_embeds_t5, negative_prompt_embeds_llama3 = embeddings[\"negative_prompt_embeds\"]\n",
    "    negative_pooled_prompt_embeds = embeddings[\"negative_pooled_prompt_embeds\"]\n",
    "\n",
    "\n",
    "    latents = pipe(\n",
    "        prompt_embeds_t5=prompt_embeds_t5,\n",
    "        prompt_embeds_llama3=prompt_embeds_llama3, \n",
    "        pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "        negative_prompt_embeds_t5=negative_prompt_embeds_t5,\n",
    "        negative_prompt_embeds_llama3=negative_prompt_embeds_llama3, \n",
    "        negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=torch.Generator(device).manual_seed(0),\n",
    "        output_type=\"latent\",\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "    \n",
    "    print(pipe)\n",
    "    del pipe\n",
    "    flush(device)\n",
    "\n",
    "    return latents\n",
    "\n",
    "# --- Main Execution ---\n",
    "time_gen = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass negative_prompt to the encoding function\n",
    "    embeddings = encode_prompt(prompt, negative_prompt, repo_id, llama_repo_id, device=device, dtype=torch_dtype)\n",
    "\n",
    "\n",
    "latents = denoise(embeddings, device=device, dtype=torch_dtype) \n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(repo_id, subfolder=\"vae\", torch_dtype=torch_dtype).to(device)\n",
    "\n",
    "latents = (latents / vae.config.scaling_factor) + vae.config.shift_factor\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents, return_dict=False)[0]\n",
    "\n",
    "\n",
    "vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "image_processor = VaeImageProcessor(vae_scale_factor=vae_scale_factor * 2)\n",
    "image = image_processor.postprocess(image, output_type=\"pil\")[0]\n",
    "\n",
    "filename = f\"hidream_cfg{guidance_scale}_steps_{num_inference_steps}{str(int(time.time()))}.png\"\n",
    "result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "image.save(filename)\n",
    "os.startfile(filename)\n",
    "print(f\"   ... Generated in {time.time() - time_gen:.2f} secs, mem/temp/use: {result.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29484cb9-7fb7-44b8-a388-4f53c0700de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
