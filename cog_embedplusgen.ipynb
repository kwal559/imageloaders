
import time, os, torch, subprocess, gc
from diffusers import Lumina2Pipeline


def flush():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()

def bytes_to_giga_bytes(bytes):
    return bytes / 1024 / 1024 / 1024

model_path = "Alpha-VLLM/Lumina-Image-2.0"
device, dtype = "cuda", torch.bfloat16

pipe = Lumina2Pipeline.from_pretrained(model_path, torch_dtype=dtype).to(device)

flush()

prompt = """
Sorceress with vibrant, flowing hair resembling swirling dragons and iridescent colors flow like a river of gold and silver with rich, jewel-toned hues and metallic accents, natural clarity, sharp focus, intricate detail, expressive, rich deep aesthetic, epic composition.
"""
negative_prompt = "cartoon, anime, poor quality, poor clarity, ugly, jpeg artifacts, cropped, lowres, error, out of frame, watermark"

system_prompt = "You are an art expert with emphasis in whimsical photorealistic creations, please convey intricate details with the highest degree of aesthetics: "            

guidance_scale = 4.0
num_inference_steps = 50
width = 1536
height = 640
cfg_trunc_ratio = 1.5
cfg_normalization = True

start_time = time.time()
image = pipe(
    system_prompt=system_prompt, prompt=prompt, negative_prompt=negative_prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps, width=width, height=height, cfg_trunc_ratio=cfg_trunc_ratio, cfg_normalization=cfg_normalization).images[0]

print(f"Generated in {time.time() - start_time:.2f} secs - {prompt}")
filename = f"lumina2_cfg_{guidance_scale}_steps_{num_inference_steps}_{str(int(time.time()))}.png"
result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)
image.save(filename);os.startfile(filename);print(f"   ... Generated in {time.time() - start_time:.2f} secs, mem/temp/use: {result}   ... Max mem allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB")

