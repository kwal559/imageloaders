{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e803f990-1764-4019-a83c-a2a561644da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "A photorealistic close-up of a single, iridescent hummingbird hovering mid-air, its wings a blur of sapphire and emerald, drinking nectar from a luminous, bioluminescent flower that emits soft, swirling particles of golden light. The background is a hyper-detailed, otherworldly jungle at twilight, with colossal, crystalline trees reflecting a nebula-filled sky. In the foreground, a single dewdrop clings precariously to a spiderweb woven with threads of pure silver. The overall atmosphere should be one of serene magic and vibrant detail plus perfect clarity, sharp focus, intricate detail, expressive style, a rich deep aesthetic, overall epic composition.\n",
    "\"\"\"\n",
    "negative_prompt = \"cartoon, anime, poor quality, poor clarity, ugly, jpeg artifacts, cropped, lowres, error, out of frame, watermark\"\n",
    "\n",
    "guidance_scale=4\n",
    "num_inference_steps=20\n",
    "width=1536\n",
    "height=640\n",
    "\n",
    "model_id =  \"THUDM/CogView4-6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05afa5be-06f5-4b39-9213-88b93c088063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d66e4cc439a442ca7d5e47228457693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30684c87e8f84974804412348c8f82d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Prompts embeded.. 18.45 seconds, Max vram: 16.47 GB\n",
      "   ... Prompt shape torch.Size([1, 144, 4096]) ... Negative shape torch.Size([1, 32, 4096])\n",
      "   ... Generating Image..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079b7752d5ec4c29870bc041ede66269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de36dbd6d21e40d9b407c041764dac8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a3804209ef450fbd1a1bef94d8b687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ... Generated in 29.43 secs, mem/temp/use: 1791 MiB, 54, 100 %\n",
      "   ... Max mem allocated: 17.09 GB\n"
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "import torch, time, gc, os, subprocess\n",
    "\n",
    "def flush():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "def bytes_to_giga_bytes(bytes):\n",
    "    return bytes / 1024 / 1024 / 1024\n",
    "device, dtype, time_start = \"cuda\", torch.bfloat16, time.time()\n",
    "emb_prompts = diffusers.DiffusionPipeline.from_pretrained(model_id, transformer=None, vae=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    (prompt_embeds, negative_prompt_embeds) = emb_prompts.encode_prompt(prompt=prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "del emb_prompts\n",
    "flush()\n",
    "print(f\"   ... Prompts embeded.. {time.time() - time_start:.2f} seconds, Max vram: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB\\n   ... Prompt shape {prompt_embeds.shape} ... Negative shape {negative_prompt_embeds.shape}\\n   ... Generating Image..\")\n",
    "\n",
    "time_gen = time.time()\n",
    "pipeline = diffusers.DiffusionPipeline.from_pretrained(model_id, text_encoder=None, tokenizer=None, torch_dtype=dtype).to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    image = pipeline(prompt_embeds=prompt_embeds.to(device).to(dtype), negative_prompt_embeds=negative_prompt_embeds.to(device).to(dtype), guidance_scale=guidance_scale, num_inference_steps=num_inference_steps, width=width, height=height).images[0]\n",
    "\n",
    "del pipeline\n",
    "flush()\n",
    "filename = f\"cog4_cfg_{guidance_scale}_steps_{num_inference_steps}_{str(int(time.time()))}.png\"\n",
    "result = subprocess.check_output(['nvidia-smi', '--query-gpu=memory.used,temperature.gpu,utilization.gpu', '--format=csv,noheader'], encoding='utf-8', timeout=1.0)\n",
    "image.save(filename);os.startfile(filename);print(f\"   ... Generated in {time.time() - time_gen:.2f} secs, mem/temp/use: {result}   ... Max mem allocated: {bytes_to_giga_bytes(torch.cuda.max_memory_allocated()):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e78000-3bb4-4326-bc85-75587ece5273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31578a4-c77e-4d17-817e-31cb04c2c711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
